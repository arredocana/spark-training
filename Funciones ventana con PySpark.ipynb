{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones ventana en PySpark\n",
    "\n",
    "Las funciones ventana se utilizan para calcular resultados como el rango, el número de fila, etc., sobre un rango de filas de entrada y devuelven un valor para cada una de ellas. Son útiles cuando necesitamos hacer operaciones de agregación en un marco de ventana específico en las columnas de un DataFrame.\n",
    "\n",
    "PySpark soporta 3 tipos de funciones ventana:\n",
    "\n",
    "* De ranking\n",
    "* Analíticas\n",
    "* De agregación\n",
    "\n",
    "Para desarrollar una operación en un grupo primero necesitamos particionar los datos usando el método `Window.partitionBy()` y para las funciones de ranking `row_number()` y `rank()` también necesitamos ordenar por los datos particionados usando la cláusula `orderBy`.\n",
    "\n",
    "A continuación vamos a cómo funcionan con un DataFrame de ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries needed\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://EM2020002559.bosonit.local:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2b77d70ddf0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print SparkSession\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = (\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100)\n",
    ")\n",
    "\n",
    "columns = [\"employee_name\", \"department\", \"salary\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones ventana de ranking\n",
    "\n",
    "### row_number()\n",
    "\n",
    "La función de ventana `row_number()` se utiliza para enumerar de forma secuencial empezando por 1 el grupo de filas pertenecientes a cada partición de la ventana. \n",
    "\n",
    "En este ejemplo particionamos por la columna `department` e indicamos que enumere por orden ascendente de `salario` cada departamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------+----------+------+----------+\n|employee_name|department|salary|row_number|\n+-------------+----------+------+----------+\n|        James|     Sales|  3000|         1|\n|        James|     Sales|  3000|         2|\n|       Robert|     Sales|  4100|         3|\n|         Saif|     Sales|  4100|         4|\n|      Michael|     Sales|  4600|         5|\n|        Maria|   Finance|  3000|         1|\n|        Scott|   Finance|  3300|         2|\n|          Jen|   Finance|  3900|         3|\n|        Kumar| Marketing|  2000|         1|\n|         Jeff| Marketing|  3000|         2|\n+-------------+----------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "windowSpec = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "\n",
    "df.withColumn(\"row_number\", row_number().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rank()\n",
    "\n",
    "La función de ventana `rank()` se utiliza para proporcionar una clasificación dentro de una ventana de partición. En caso de empate entre valores de una misma partición se les asigna el mismo rango y se omite los rangos siguientes. Es decir, si tenemos 3 elementos en el rango 2, el siguiente rango sería 5.\n",
    "\n",
    "En nuestro ejemplo vemos que en el grupo de filas del departamento <b>Ventas</b>, el empleado Robert y Saif tienen el mismo salario luego se clasifican ambos con el rango 3 en lugar de 3 y 4. \n",
    "\n",
    "El rango 4 se salta y el siguiente empleado Michael se clasifica con el rango 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------+----------+------+----+\n|employee_name|department|salary|rank|\n+-------------+----------+------+----+\n|        James|     Sales|  3000|   1|\n|        James|     Sales|  3000|   1|\n|       Robert|     Sales|  4100|   3|\n|         Saif|     Sales|  4100|   3|\n|      Michael|     Sales|  4600|   5|\n|        Maria|   Finance|  3000|   1|\n|        Scott|   Finance|  3300|   2|\n|          Jen|   Finance|  3900|   3|\n|        Kumar| Marketing|  2000|   1|\n|         Jeff| Marketing|  3000|   2|\n+-------------+----------+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"rank\", rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dense_rank()\n",
    "\n",
    "La función de ventana `dense_rank()` se comporta igual que `rank()` salvo que los rangos son consecutivos. Es decir, en caso de empate no se salta ningún rango."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------+----------+------+----------+\n|employee_name|department|salary|dense_rank|\n+-------------+----------+------+----------+\n|        James|     Sales|  3000|         1|\n|        James|     Sales|  3000|         1|\n|       Robert|     Sales|  4100|         2|\n|         Saif|     Sales|  4100|         2|\n|      Michael|     Sales|  4600|         3|\n|        Maria|   Finance|  3000|         1|\n|        Scott|   Finance|  3300|         2|\n|          Jen|   Finance|  3900|         3|\n|        Kumar| Marketing|  2000|         1|\n|         Jeff| Marketing|  3000|         2|\n+-------------+----------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"dense_rank\", dense_rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones ventana de agregación\n",
    "\n",
    "Las funciones de agregación operan sobre un grupo de filas y calculan un único valor de retorno para cada grupo. Estas funciones se pueden utilizar también como función de ventana.\n",
    "\n",
    "En este ejemplo vamos a ver como calcular el salario máximo, mínimo y total para cada departamento utilizando funciones ventana en lugar de utilizar la función `groupBy()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+------+-----+----+----+\n|department|   avg|  sum| min| max|\n+----------+------+-----+----+----+\n|     Sales|3760.0|18800|3000|4600|\n|   Finance|3400.0|10200|3000|3900|\n| Marketing|2500.0| 5000|2000|3000|\n+----------+------+-----+----+----+\n\n"
     ]
    }
   ],
   "source": [
    "# Using window aggregate functions\n",
    "windowSpecAgg = Window.partitionBy(\"department\")\n",
    "\n",
    "(\n",
    "  df\n",
    "  .withColumn(\"row\", row_number().over(windowSpec))\n",
    "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg))\n",
    "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg))\n",
    "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg))\n",
    "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg))\n",
    "  .where(col(\"row\")==1)\n",
    "  .select(\"department\",\"avg\",\"sum\",\"min\",\"max\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+------+-----+----+----+\n|department|   avg|  sum| min| max|\n+----------+------+-----+----+----+\n|     Sales|3760.0|18800|3000|4600|\n|   Finance|3400.0|10200|3000|3900|\n| Marketing|2500.0| 5000|2000|3000|\n+----------+------+-----+----+----+\n\n"
     ]
    }
   ],
   "source": [
    "# Using groupBy function\n",
    "(\n",
    "    df\n",
    "    .groupBy(\"department\")\n",
    "    .agg(\n",
    "        avg(\"salary\").alias(\"avg\"),\n",
    "        sum(\"salary\").alias(\"sum\"),\n",
    "        min(\"salary\").alias(\"min\"),\n",
    "        max(\"salary\").alias(\"max\")\n",
    "    )\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso práctico de uso"
   ]
  },
  {
   "source": [
    "A continuación vamos a ver un caso práctico de uso de las funciones ventana en una base de datos con más de 5 millones de registros.\n",
    "\n",
    "La base de datos se tiene que descargar de esta [dirección]() y cargar en MySQL. \n",
    "\n",
    "Posteriormente, leeremos las tablas con Spark y realizaremos una serie de transformaciones para obtener un dataset limpio que guardaremos en formato parquet y utilizaremos para la práctica de las funciones ventanas."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+-----------+\n",
      "|database|tableName   |isTemporary|\n",
      "+--------+------------+-----------+\n",
      "|        |departments |true       |\n",
      "|        |dept_emp    |true       |\n",
      "|        |dept_manager|true       |\n",
      "|        |employees   |true       |\n",
      "|        |salaries    |true       |\n",
      "|        |titles      |true       |\n",
      "+--------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#https://dev.mysql.com/doc/employee/en/\n",
    "    \n",
    "tables = [\"employees\",\"salaries\", \"titles\", \"dept_emp\", \"departments\", \"dept_manager\"]\n",
    "\n",
    "for table in tables:\n",
    "    (spark.read\n",
    "     .format(\"jdbc\")\n",
    "     .option(\"url\", \"jdbc:mysql://localhost:3306/employees\")\n",
    "     .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    "     .option(\"dbtable\", table)\n",
    "     .option(\"user\", \"root\")\n",
    "     .option(\"password\", \"password\")\n",
    "     .load()) \\\n",
    "    .createOrReplaceTempView(table)\n",
    "    \n",
    "spark.sql(\"SHOW TABLES\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining tables with SQL\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        e.emp_no,\n",
    "        e.first_name || \" \" || e.last_name AS name,\n",
    "        t.title,\n",
    "        d.dept_name AS department,\n",
    "        s.salary,\n",
    "        s.from_date,\n",
    "        s.to_date\n",
    "    FROM employees AS e\n",
    "    INNER JOIN titles AS t ON t.emp_no = e.emp_no\n",
    "    INNER JOIN salaries AS s ON s.emp_no = e.emp_no\n",
    "    INNER JOIN dept_emp AS de ON de.emp_no = e.emp_no\n",
    "    INNER JOIN departments AS d ON d.dept_no = de.dept_no\n",
    "\"\"\"\n",
    "\n",
    "# Create DataFrame from query\n",
    "employeeDF = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+----------------+-----------+------+----------+----------+\n",
      "|emp_no|             name|           title| department|salary| from_date|   to_date|\n",
      "+------+-----------------+----------------+-----------+------+----------+----------+\n",
      "| 10206| Alassane Iwayama|Technique Leader|Development| 40000|1988-04-19|1989-04-19|\n",
      "| 10206| Alassane Iwayama|Technique Leader|Development| 43519|1989-04-19|1990-04-19|\n",
      "| 10206| Alassane Iwayama|Technique Leader|Development| 46265|1990-04-19|1991-04-19|\n",
      "| 10206| Alassane Iwayama|Technique Leader|Development| 46865|1991-04-19|1992-04-18|\n",
      "| 10206| Alassane Iwayama|Technique Leader|Development| 47837|1992-04-18|1993-04-18|\n",
      "| 10206| Alassane Iwayama|Technique Leader|Development| 52042|1993-04-18|1994-04-18|\n",
      "| 10206| Alassane Iwayama|Technique Leader|Development| 52370|1994-04-18|1995-04-18|\n",
      "| 10206| Alassane Iwayama|Technique Leader|Development| 53202|1995-04-18|1996-04-17|\n",
      "| 10206| Alassane Iwayama|Technique Leader|Development| 56087|1996-04-17|1997-04-17|\n",
      "| 10206| Alassane Iwayama|Technique Leader|Development| 59252|1997-04-17|1998-04-17|\n",
      "| 10206| Alassane Iwayama|Technique Leader|Development| 62716|1998-04-17|1999-04-17|\n",
      "| 10206| Alassane Iwayama|Technique Leader|Development| 67137|1999-04-17|2000-04-16|\n",
      "| 10206| Alassane Iwayama|Technique Leader|Development| 67944|2000-04-16|2001-04-16|\n",
      "| 10206| Alassane Iwayama|Technique Leader|Development| 67588|2001-04-16|2002-04-16|\n",
      "| 10206| Alassane Iwayama|Technique Leader|Development| 71052|2002-04-16|9999-01-01|\n",
      "| 10623|Aleksander Danlos|        Engineer|Development| 60268|1992-01-15|1993-01-14|\n",
      "| 10623|Aleksander Danlos|        Engineer|Development| 61073|1993-01-14|1994-01-14|\n",
      "| 10623|Aleksander Danlos|        Engineer|Development| 63219|1994-01-14|1995-01-14|\n",
      "| 10623|Aleksander Danlos|        Engineer|Development| 67084|1995-01-14|1996-01-14|\n",
      "| 10623|Aleksander Danlos|        Engineer|Development| 66979|1996-01-14|1997-01-13|\n",
      "+------+-----------------+----------------+-----------+------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DataFrame to Parquet file\n",
    "employeeDF.write.mode(\"overwrite\").parquet(\"data/employee.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos nuestro dataset en formato parquet vamos a leerlo con Spark y exponer el ejemplo donde podemos hacer uso de las funciones ventana:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+----------------+-----------+------+----------+----------+\n",
      "|emp_no|            name|           title| department|salary| from_date|   to_date|\n",
      "+------+----------------+----------------+-----------+------+----------+----------+\n",
      "| 10206|Alassane Iwayama|Technique Leader|Development| 40000|1988-04-19|1989-04-19|\n",
      "|450881|Irena Lieberherr|        Engineer|Development| 59264|1997-01-22|1998-01-22|\n",
      "| 10206|Alassane Iwayama|Technique Leader|Development| 43519|1989-04-19|1990-04-19|\n",
      "|450881|Irena Lieberherr|        Engineer|Development| 60536|1998-01-22|1999-01-22|\n",
      "| 10206|Alassane Iwayama|Technique Leader|Development| 46265|1990-04-19|1991-04-19|\n",
      "|450881|Irena Lieberherr|        Engineer|Development| 60170|1999-01-22|2000-01-22|\n",
      "| 10206|Alassane Iwayama|Technique Leader|Development| 46865|1991-04-19|1992-04-18|\n",
      "|450881|Irena Lieberherr|        Engineer|Development| 61146|2000-01-22|2001-01-21|\n",
      "| 10206|Alassane Iwayama|Technique Leader|Development| 47837|1992-04-18|1993-04-18|\n",
      "|450881|Irena Lieberherr|        Engineer|Development| 64394|2001-01-21|2002-01-21|\n",
      "| 10206|Alassane Iwayama|Technique Leader|Development| 52042|1993-04-18|1994-04-18|\n",
      "|450881|Irena Lieberherr|        Engineer|Development| 64029|2002-01-21|9999-01-01|\n",
      "| 10206|Alassane Iwayama|Technique Leader|Development| 52370|1994-04-18|1995-04-18|\n",
      "|450881|Irena Lieberherr| Senior Engineer|Development| 40000|1991-01-24|1992-01-24|\n",
      "| 10206|Alassane Iwayama|Technique Leader|Development| 53202|1995-04-18|1996-04-17|\n",
      "|450881|Irena Lieberherr| Senior Engineer|Development| 44053|1992-01-24|1993-01-23|\n",
      "| 10206|Alassane Iwayama|Technique Leader|Development| 56087|1996-04-17|1997-04-17|\n",
      "|450881|Irena Lieberherr| Senior Engineer|Development| 48377|1993-01-23|1994-01-23|\n",
      "| 10206|Alassane Iwayama|Technique Leader|Development| 59252|1997-04-17|1998-04-17|\n",
      "|450881|Irena Lieberherr| Senior Engineer|Development| 48172|1994-01-23|1995-01-23|\n",
      "+------+----------------+----------------+-----------+------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDF_raw = spark.read.parquet(\"data/employee.parquet\")\n",
    "\n",
    "employeeDF_raw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5124191"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of records\n",
    "employeeDF_raw.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La base de datos contiene un registro de los empleados de una empresa donde se almacena su puesto de trabajo, salario, departamento y la fecha de inicio y fin en cada puesto. Por lo tanto, cada empleado puede aparecer varias veces en la base de datos según su evolución dentro de la empresa.\n",
    "\n",
    "Por ejemplo, si ordenamos el DataFrame por el id del empleado y la fecha del puesto más reciente vemos que el empleado nº 10001, Georgi Facello, trabaja en el departamento Development como Senior Engineer desde el 22 de Junio de 2002 hasta la fecha de hoy (se indica con el año 9999):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+---------------+-----------+------+----------+----------+\n",
      "|emp_no|          name|          title| department|salary| from_date|   to_date|\n",
      "+------+--------------+---------------+-----------+------+----------+----------+\n",
      "| 10001|Georgi Facello|Senior Engineer|Development| 88958|2002-06-22|9999-01-01|\n",
      "| 10001|Georgi Facello|Senior Engineer|Development| 85097|2001-06-22|2002-06-22|\n",
      "| 10001|Georgi Facello|Senior Engineer|Development| 85112|2000-06-22|2001-06-22|\n",
      "| 10001|Georgi Facello|Senior Engineer|Development| 84917|1999-06-23|2000-06-22|\n",
      "| 10001|Georgi Facello|Senior Engineer|Development| 81097|1998-06-23|1999-06-23|\n",
      "| 10001|Georgi Facello|Senior Engineer|Development| 81025|1997-06-23|1998-06-23|\n",
      "| 10001|Georgi Facello|Senior Engineer|Development| 80013|1996-06-23|1997-06-23|\n",
      "| 10001|Georgi Facello|Senior Engineer|Development| 76884|1995-06-24|1996-06-23|\n",
      "| 10001|Georgi Facello|Senior Engineer|Development| 75994|1994-06-24|1995-06-24|\n",
      "| 10001|Georgi Facello|Senior Engineer|Development| 75286|1993-06-24|1994-06-24|\n",
      "| 10001|Georgi Facello|Senior Engineer|Development| 74333|1992-06-24|1993-06-24|\n",
      "| 10001|Georgi Facello|Senior Engineer|Development| 71046|1991-06-25|1992-06-24|\n",
      "| 10001|Georgi Facello|Senior Engineer|Development| 66961|1990-06-25|1991-06-25|\n",
      "| 10001|Georgi Facello|Senior Engineer|Development| 66596|1989-06-25|1990-06-25|\n",
      "| 10001|Georgi Facello|Senior Engineer|Development| 66074|1988-06-25|1989-06-25|\n",
      "| 10001|Georgi Facello|Senior Engineer|Development| 62102|1987-06-26|1988-06-25|\n",
      "| 10001|Georgi Facello|Senior Engineer|Development| 60117|1986-06-26|1987-06-26|\n",
      "| 10002|Bezalel Simmel|          Staff|      Sales| 72527|2001-08-02|9999-01-01|\n",
      "| 10002|Bezalel Simmel|          Staff|      Sales| 71963|2000-08-02|2001-08-02|\n",
      "| 10002|Bezalel Simmel|          Staff|      Sales| 69366|1999-08-03|2000-08-02|\n",
      "+------+--------------+---------------+-----------+------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDF_raw.orderBy(\"emp_no\", desc(\"to_date\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos utilizar la función de ventana `row_number()` para quedarnos con el registro de cada empleado con su puesto de trabajo más reciente.\n",
    "\n",
    "Si particionamos por id de empleado y ordenamos por fecha descendiente observamos que la primera fila de cada partición contiene el registro más reciente para cada empleado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+----------------+---------------+------+----------+----------+----------+\n",
      "|emp_no|            name|           title|     department|salary| from_date|   to_date|row_number|\n",
      "+------+----------------+----------------+---------------+------+----------+----------+----------+\n",
      "| 10206|Alassane Iwayama|Technique Leader|    Development| 71052|2002-04-16|9999-01-01|         1|\n",
      "| 10206|Alassane Iwayama|Technique Leader|    Development| 67588|2001-04-16|2002-04-16|         2|\n",
      "| 10206|Alassane Iwayama|Technique Leader|    Development| 67944|2000-04-16|2001-04-16|         3|\n",
      "| 10206|Alassane Iwayama|Technique Leader|    Development| 67137|1999-04-17|2000-04-16|         4|\n",
      "| 10206|Alassane Iwayama|Technique Leader|    Development| 62716|1998-04-17|1999-04-17|         5|\n",
      "| 10206|Alassane Iwayama|Technique Leader|    Development| 59252|1997-04-17|1998-04-17|         6|\n",
      "| 10206|Alassane Iwayama|Technique Leader|    Development| 56087|1996-04-17|1997-04-17|         7|\n",
      "| 10206|Alassane Iwayama|Technique Leader|    Development| 53202|1995-04-18|1996-04-17|         8|\n",
      "| 10206|Alassane Iwayama|Technique Leader|    Development| 52370|1994-04-18|1995-04-18|         9|\n",
      "| 10206|Alassane Iwayama|Technique Leader|    Development| 52042|1993-04-18|1994-04-18|        10|\n",
      "| 10206|Alassane Iwayama|Technique Leader|    Development| 47837|1992-04-18|1993-04-18|        11|\n",
      "| 10206|Alassane Iwayama|Technique Leader|    Development| 46865|1991-04-19|1992-04-18|        12|\n",
      "| 10206|Alassane Iwayama|Technique Leader|    Development| 46265|1990-04-19|1991-04-19|        13|\n",
      "| 10206|Alassane Iwayama|Technique Leader|    Development| 43519|1989-04-19|1990-04-19|        14|\n",
      "| 10206|Alassane Iwayama|Technique Leader|    Development| 40000|1988-04-19|1989-04-19|        15|\n",
      "| 10362| Shalesh dAstous|    Senior Staff|Human Resources| 54987|1996-10-31|1997-07-16|         1|\n",
      "| 10362| Shalesh dAstous|    Senior Staff|Human Resources| 53121|1995-11-01|1996-10-31|         2|\n",
      "| 10362| Shalesh dAstous|    Senior Staff|Human Resources| 48889|1994-11-01|1995-11-01|         3|\n",
      "| 10362| Shalesh dAstous|    Senior Staff|Human Resources| 49296|1993-11-01|1994-11-01|         4|\n",
      "| 10362| Shalesh dAstous|    Senior Staff|Human Resources| 45546|1992-11-01|1993-11-01|         5|\n",
      "+------+----------------+----------------+---------------+------+----------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpec = Window.partitionBy(\"emp_no\").orderBy(desc(\"to_date\"))\n",
    "\n",
    "employeeDF_raw.withColumn(\"row_number\", row_number().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta manera usando la función de ventana `row_number()` podemos filtrar el DataFrame por la primera fila de cada partición para así quedarnos con el registro más reciente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+----------------+------------------+------+----------+----------+----------+\n",
      "|emp_no|              name|           title|        department|salary| from_date|   to_date|row_number|\n",
      "+------+------------------+----------------+------------------+------+----------+----------+----------+\n",
      "| 10206|  Alassane Iwayama|Technique Leader|       Development| 71052|2002-04-16|9999-01-01|         1|\n",
      "| 10362|   Shalesh dAstous|    Senior Staff|   Human Resources| 54987|1996-10-31|1997-07-16|         1|\n",
      "| 10623| Aleksander Danlos|        Engineer|       Development| 86399|2002-01-12|9999-01-01|         1|\n",
      "| 10817|       Uri Rullman|    Senior Staff|             Sales| 78202|2001-12-23|9999-01-01|         1|\n",
      "| 11033|      Shushma Bahk|        Engineer|       Development| 75271|2002-03-11|9999-01-01|         1|\n",
      "| 11141| Vasiliy Kermarrec|        Engineer|       Development| 57809|2002-02-10|9999-01-01|         1|\n",
      "| 11317| Shigeaki Hagimont|    Senior Staff|       Development| 55544|2001-09-06|9999-01-01|         1|\n",
      "| 11458|   Stevo Chenoweth|    Senior Staff|          Research| 82731|2001-10-02|9999-01-01|         1|\n",
      "| 11748|   Lihong Massonet|        Engineer|       Development| 70230|2002-07-23|9999-01-01|         1|\n",
      "| 11858|      Slavian Baik|           Staff|           Finance| 47998|2001-08-14|9999-01-01|         1|\n",
      "| 12027|  Zhanqiu Vuskovic|Technique Leader|       Development| 71281|2001-11-19|9999-01-01|         1|\n",
      "| 12046|       Xiong Ranum|           Staff|             Sales| 76043|1998-01-12|1998-09-26|         1|\n",
      "| 12799|    Tiina Businaro|    Senior Staff|   Human Resources| 59831|2001-12-23|9999-01-01|         1|\n",
      "| 12940|   Odinaldo Farrar|        Engineer|       Development| 85425|2001-12-08|9999-01-01|         1|\n",
      "| 13285|   Uinam Lienhardt| Senior Engineer|       Development| 48494|2000-06-24|2000-11-02|         1|\n",
      "| 13289|        Berni Baer|    Senior Staff|         Marketing| 85488|2001-09-20|9999-01-01|         1|\n",
      "| 13623|    Breannda Yeung|        Engineer|Quality Management| 51145|2001-10-27|9999-01-01|         1|\n",
      "| 13832| Hidefumi Siepmann|           Staff|        Production| 77530|1998-10-05|1999-05-05|         1|\n",
      "| 13840|       Remco Demke|        Engineer|       Development| 41453|1999-12-12|1999-12-30|         1|\n",
      "| 14450|Fumitaka Prochazka| Senior Engineer|       Development| 75419|1996-04-23|1996-12-30|         1|\n",
      "+------+------------------+----------------+------------------+------+----------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpec = Window.partitionBy(\"emp_no\").orderBy(desc(\"to_date\"))\n",
    "\n",
    "employeeDF = employeeDF_raw.withColumn(\"row_number\", row_number().over(windowSpec))\\\n",
    "    .where(col(\"row_number\")==1)\n",
    "\n",
    "employeeDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente eliminamos la columna `row_number` y obtenemos el DataFrame definitivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+----------------+---------------+------+----------+----------+\n",
      "|emp_no|             name|           title|     department|salary| from_date|   to_date|\n",
      "+------+-----------------+----------------+---------------+------+----------+----------+\n",
      "| 10206| Alassane Iwayama|Technique Leader|    Development| 71052|2002-04-16|9999-01-01|\n",
      "| 10362|  Shalesh dAstous|    Senior Staff|Human Resources| 54987|1996-10-31|1997-07-16|\n",
      "| 10623|Aleksander Danlos|        Engineer|    Development| 86399|2002-01-12|9999-01-01|\n",
      "| 10817|      Uri Rullman|    Senior Staff|          Sales| 78202|2001-12-23|9999-01-01|\n",
      "| 11033|     Shushma Bahk|        Engineer|    Development| 75271|2002-03-11|9999-01-01|\n",
      "+------+-----------------+----------------+---------------+------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDF = employeeDF.drop(\"row_number\")\n",
    "\n",
    "employeeDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300024"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeeDF.count()"
   ]
  },
  {
   "source": [
    "## Otros ejemplos"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "El siguiente dataframe muestra un extracto (ficticio) de una tabla de contratos de tarjetas de crédito que almacena datos de forma mensual, siendo el campo `closing_date` la fecha a la que hacen referencia los datos publicados.\n",
    "\n",
    "Para cada contrato se desea obtener los datos más recientes registrados en la tabla."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+------------+-----------+\n|contract_id|closing_date|payment_fee|\n+-----------+------------+-----------+\n|       0001|     2021-04|          Q|\n|       0001|     2021-05|          Q|\n|       0001|     2021-06|       null|\n|       0002|     2021-04|          N|\n|       0003|     2021-04|          M|\n|       0003|     2021-05|          S|\n|       0004|     2021-01|          N|\n|       0004|     2021-08|          N|\n+-----------+------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "credit_df = spark.createDataFrame([\n",
    "    ('0001', '2021-04', 'Q'),\n",
    "    ('0001', '2021-05', 'Q'),\n",
    "    ('0001', '2021-06', None),\n",
    "    ('0002', '2021-04', 'N'),\n",
    "    ('0003', '2021-04', 'M'),\n",
    "    ('0003', '2021-05', 'S'),\n",
    "    ('0004', '2021-01', 'N'),\n",
    "    ('0004', '2021-08', 'N')],\n",
    "    ['contract_id', 'closing_date', 'payment_fee']\n",
    ")\n",
    "credit_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+------------+-----------+\n|contract_id|closing_date|payment_fee|\n+-----------+------------+-----------+\n|       0001|     2021-06|          Q|\n|       0001|     2021-06|       null|\n|       0002|     2021-04|          N|\n|       0003|     2021-05|          M|\n|       0003|     2021-05|          S|\n|       0004|     2021-08|          N|\n+-----------+------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy('contract_id')\n",
    "(\n",
    "    credit_df.select(\n",
    "        'contract_id', \n",
    "        max('closing_date').over(w).alias('closing_date'),\n",
    "        'payment_fee')\n",
    "    .distinct()\n",
    "    .orderBy('contract_id')\n",
    ").show()"
   ]
  },
  {
   "source": [
    "Utilizando la misma tabla se quiere comprobar si un contrato ha generado comisiones en algun momento desde que está registrado en la base de datos.\n",
    "\n",
    "Para ello se necesita analizar si el campo `payment_fee` es distinto del valor `N` (incluyendo valores nulos)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+------------+-----------+\n|contract_id|closing_date|payment_fee|\n+-----------+------------+-----------+\n|       0001|     2021-04|          Q|\n|       0001|     2021-05|          Q|\n|       0001|     2021-06|       null|\n|       0002|     2021-04|          N|\n|       0003|     2021-04|          M|\n|       0003|     2021-05|          S|\n|       0004|     2021-01|          N|\n|       0004|     2021-08|          N|\n+-----------+------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "credit_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+------------+-----------+-------------+\n|contract_id|closing_date|payment_fee|fee_per_month|\n+-----------+------------+-----------+-------------+\n|       0001|     2021-04|          Q|            1|\n|       0001|     2021-05|          Q|            1|\n|       0001|     2021-06|       null|            1|\n|       0002|     2021-04|          N|            0|\n|       0003|     2021-04|          M|            1|\n|       0003|     2021-05|          S|            1|\n|       0004|     2021-01|          N|            0|\n|       0004|     2021-08|          N|            0|\n+-----------+------------+-----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "tmp_df = credit_df.withColumn('fee_per_month', when(col('payment_fee') == 'N', 0).otherwise(1))\n",
    "\n",
    "tmp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+-------+\n|contract_id|has_fee|\n+-----------+-------+\n|       0001|      1|\n|       0002|      0|\n|       0003|      1|\n|       0004|      0|\n+-----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy('contract_id')\n",
    "\n",
    "( \n",
    "    tmp_df.select(\n",
    "        'contract_id',\n",
    "        when(sum(col('fee_per_month')).over(w) > 0, 1).otherwise(0).alias('has_fee')\n",
    "    )\n",
    "    .distinct()\n",
    "    .orderBy('contract_id')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "* https://sparkbyexamples.com/pyspark/pyspark-window-functions/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "a49a3eaf43f92c8c35fbc05f2ff363591d97c0b1992a66140feb5af3adc6463c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Apache Spark's Structured APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find path to PySpark\n",
    "# import findspark\n",
    "# findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PySpark and libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2285c4da2b0>"
      ],
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://EM2020002559.bosonit.local:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.0.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>chapter3</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# Build a SparkSession\n",
    "spark = SparkSession.builder.appName(\"chapter3\").getOrCreate()\n",
    "\n",
    "# Print SparkSession\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+---------+-----------+\n|database|tableName|isTemporary|\n+--------+---------+-----------+\n+--------+---------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The DataFrame API\n",
    "\n",
    "## Schemas and Creating DataFrames\n",
    "\n",
    "A schema in Spark defines the column names and associated data types for a DataFrame. Most often, schemas come into play when you are reading structured data from an **external data source**.\n",
    "\n",
    "Defining a schema up front as opposed to taking a schema-on-read approach offers three benefits:\n",
    "\n",
    "* You relieve Spark from the onus of inferring data types.\n",
    "* You prevent Spark from creating a separate job just to read a large portion of your file to ascertain the schema, which for a large data file can be expensive and time-consuming.\n",
    "* You can detect errors early if data doesn’t match the schema.\n",
    "\n",
    "Spark allows you to define a schema in two ways. One is to define it **programmatically**, and the other is to employ a **Data Definition Language (DDL) string**, which is much simpler and easier to read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nullable indicates if the concerned column can be null or not. It ensures that a specific column can't be null (if it's null while the nullable property is set to true, Spark will launch a `java.lang.RuntimeException` during the first action on the dataframe).\n",
    "\n",
    "Spark uses a simple set of rules to determine nullable property when creating a Dataset from a statically typed structure:\n",
    "\n",
    "* If an object of the given type can be null then its DataFrame representation is nullable.\n",
    "* If object is an Option[_] then its DataFrame representation is nullable with None considered to be SQL NULL.\n",
    "* Otherwise, it will be marked as not nullable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+-----+\n|    Authors|State|\n+-----------+-----+\n|       null|   CA|\n|Reynold Xin|   CA|\n+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# nullable – Whether the field to add should be nullable (default True)\n",
    "\n",
    "rows = [Row(None, \"CA\"), Row(\"Reynold Xin\", \"CA\")]\n",
    "\n",
    "# if it's False raises ValueError exception: field Authors: This field is not nullable, but got None\n",
    "schema = StructType([\n",
    "    StructField(\"Authors\", StringType(), True), \n",
    "    StructField(\"State\", StringType(), True)\n",
    "])\n",
    "\n",
    "spark.createDataFrame(rows, schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for our data programmatically\n",
    "schema = StructType([\n",
    "   StructField(\"Id\", IntegerType(), False),\n",
    "   StructField(\"First\", StringType(), False),\n",
    "   StructField(\"Last\", StringType(), False),\n",
    "   StructField(\"Url\", StringType(), False),\n",
    "   StructField(\"Published\", StringType(), False),\n",
    "   StructField(\"Hits\", IntegerType(), False),\n",
    "   StructField(\"Campaigns\", ArrayType(StringType()), False)])\n",
    "\n",
    "# Define schema for our data using DDL\n",
    "schema_ddl = \"\"\"Id INT, First STRING, Last STRING, Url STRING,\n",
    "            Published STRING, Hits INT, Campaigns ARRAY<STRING>\"\"\"\n",
    "\n",
    "# Create our static data\n",
    "data = [[1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\", \"LinkedIn\"]],\n",
    "       [2, \"Brooke\",\"Wenig\",\"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\", \"LinkedIn\"]],\n",
    "       [3, \"Denny\", \"Lee\", \"https://tinyurl.3\",\"6/7/2019\",7659, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "       [4, \"Tathagata\", \"Das\",\"https://tinyurl.4\", \"5/12/2018\", 10568, [\"twitter\", \"FB\"]],\n",
    "       [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "       [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568, [\"twitter\", \"LinkedIn\"]]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n+---+---------+-------+-----------------+---------+-----+--------------------+\n|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n+---+---------+-------+-----------------+---------+-----+--------------------+\n\nroot\n |-- Id: integer (nullable = false)\n |-- First: string (nullable = false)\n |-- Last: string (nullable = false)\n |-- Url: string (nullable = false)\n |-- Published: string (nullable = false)\n |-- Hits: integer (nullable = false)\n |-- Campaigns: array (nullable = false)\n |    |-- element: string (containsNull = true)\n\nNone\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame using the schema defined\n",
    "blogs_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show the DataFrame; it should reflect our table above\n",
    "blogs_df.show()\n",
    "\n",
    "# Print the schema used by Spark to process the DataFrame\n",
    "print(blogs_df.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use this schema elsewhere in your code, simply execute `blogs_df.schema` and it will return the schema definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "StructType(List(StructField(Id,IntegerType,false),StructField(First,StringType,false),StructField(Last,StringType,false),StructField(Url,StringType,false),StructField(Published,StringType,false),StructField(Hits,IntegerType,false),StructField(Campaigns,ArrayType(StringType,true),false)))\n"
     ]
    }
   ],
   "source": [
    "print(blogs_df.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns and Expressions\n",
    "\n",
    "Named columns in DataFrames are conceptually similar to named columns in pandas or R DataFrames or in an RDBMS table: **they describe a type of field**. You can list all the columns by their names, and you can perform operations on their values using relational or computational expressions. \n",
    "\n",
    "In Spark’s supported languages, columns are objects with public methods (represented by the Column type). Like any other function in those packages, `expr()` takes arguments that Spark will parse as an expression, computing the result.\n",
    "\n",
    "You can list all the columns by their names, and you can perform operations on their values using relational or computational expressions.\n",
    "\n",
    "You can also use logical or mathematical expressions on columns. \n",
    "\n",
    "For example, you could create a simple expression using *expr(\"columnName * 5\")* or *(expr(\"columnName - 5\") > col(anothercolumnName))*, where *columnName* is a Spark type (integer, string, etc.). `expr()` is part of the `pyspark.sql.functions`.\n",
    "\n",
    "You’ll note that the Spark documentation refers to both **col** and **Column**. Column is the name of the object, while col() is a standard built-in function that returns a Column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Id', 'First', 'Last', 'Url', 'Published', 'Hits', 'Campaigns']"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# Show columns of the dataframe\n",
    "blogs_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[Id: int]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# Access a particular column with col and it returns a Column type\n",
    "blogs_df.select(F.col('Id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use an expression to compute a value\n",
    "blogs_df.select(F.expr(\"Hits\") * 2).show(2)\n",
    "blogs_df.select(F.expr(\"Hits * 2\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+\n|(Hits * 2)|\n+----------+\n|      9070|\n|     17816|\n+----------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Use col to compute value\n",
    "blogs_df.select(F.col(\"Hits\") * 2).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n| Id|    First|   Last|              Url|Published| Hits|           Campaigns|Big Hitters|\n+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|      false|\n|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|      false|\n|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|      false|\n|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|       true|\n|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|       true|\n|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|       true|\n+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Use an expression to compute big hitters for blogs\n",
    "# WithColumn adds a new column, Big Hitters, based on the conditional expression\n",
    "blogs_df.withColumn(\"Big Hitters\", F.expr(\"Hits > 10000\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------+\n|    AuthorsId|\n+-------------+\n|  JulesDamji1|\n| BrookeWenig2|\n|    DennyLee3|\n|TathagataDas4|\n+-------------+\nonly showing top 4 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Concatenate three columns, create a new column, and show the newly created concatenated column\n",
    "blogs_df.withColumn(\"AuthorsId\", (F.concat(F.expr(\"First\"), F.expr(\"Last\"), F.expr(\"Id\")))) \\\n",
    "    .select(F.col(\"AuthorsId\")) \\\n",
    "    .show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------------+\n|      AuthorsId|\n+---------------+\n|  Jules Damji 1|\n| Brooke Wenig 2|\n|    Denny Lee 3|\n|Tathagata Das 4|\n+---------------+\nonly showing top 4 rows\n\n"
     ]
    }
   ],
   "source": [
    "blogs_df.withColumn(\"AuthorsId\", (F.concat_ws(\" \", \"First\", \"Last\", \"Id\")))\\\n",
    "   .select(\"AuthorsId\")\\\n",
    "   .show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# These statements return the same value, showing that expr is the same as a col method call\n",
    "blogs_df.select(F.expr(\"Hits\")).show(2)\n",
    "blogs_df.select(F.col(\"Hits\")).show(2)\n",
    "blogs_df.select(\"Hits\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n+---+---------+-------+-----------------+---------+-----+--------------------+\n|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n+---+---------+-------+-----------------+---------+-----+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Sort by column \"Id\" in descending order\n",
    "blogs_df.sort(F.col(\"Id\"), ascending=False).show()\n",
    "#blogs_df.sort($\"Id\".desc).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both expressions sort the DataFrame column named Id in descending order: one uses an **explicit function**, `col(\"Id\")`, to return a *Column object*, while the other uses **$** before the name of the column, which is a function in Spark that converts column named Id to a Column.\n",
    "\n",
    "Eeach column is part of a row in a record and all the rows together constitute a DataFrame, which as we will see later in the chapter is really a Dataset[Row] in Scala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rows\n",
    "\n",
    "A row in Spark is a generic `Row object`, containing one or more columns. Each column may be of the same data type (e.g., integer or string), or they can have different types (integer, string, map, array, etc.). \n",
    "\n",
    "Because Row is an object in Spark and an ordered collection of fields, you can instantiate a Row in each of Spark’s supported languages and access its fields by an index starting at 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['twitter', 'LinkedIn']"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "blog_row = Row(6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", 255568, \"3/2/2015\", [\"twitter\", \"LinkedIn\"])\n",
    "# access using index for individual items\n",
    "blog_row[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Row objects can be used to create DataFrames if you need them for quick interactivity and exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------+-----+\n|      Authors|State|\n+-------------+-----+\n|Matei Zaharia|   CA|\n|  Reynold Xin|   CA|\n+-------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "rows = [Row(\"Matei Zaharia\", \"CA\"), Row(\"Reynold Xin\", \"CA\")]\n",
    "authors_df = spark.createDataFrame(rows, [\"Authors\", \"State\"])\n",
    "authors_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common DataFrames Operations\n",
    "\n",
    "Spark provides an interface, [DataFrameReader](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader), that enables you to read data into a DataFrame from myriad data sources in formats such as JSON, CSV, Parquet, Text, Avro, ORC, etc. \n",
    "\n",
    "Likewise, to write a DataFrame back to a data source in a particular format, Spark uses [DataFrameWriter](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter).\n",
    "\n",
    "### Using DataFrameReader and DataFrameWriter\n",
    "\n",
    "Reading and writing are simple in Spark because of these high-level abstractions and contributions from the community to connect to a wide variety of data sources, including common NoSQL stores, RDBMSs, streaming engines such as Apache Kafka\n",
    "and Kinesis, and more.\n",
    "\n",
    "To get started, let’s read a large CSV file containing data on San Francisco Fire Department calls.1 As noted previously, we will define a schema for this file and use the DataFrameReader class and its methods to tell Spark what to do. \n",
    "\n",
    "Because this file contains 28 columns and over 4,380,660 records, **it’s more efficient to define a schema than have Spark infer it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programmatic way to define a schema\n",
    "fire_schema = StructType([\n",
    "    StructField('CallNumber', IntegerType(), True),\n",
    "    StructField('UnitID', StringType(), True),\n",
    "    StructField('IncidentNumber', IntegerType(), True),\n",
    "    StructField('CallType', StringType(), True),\n",
    "    StructField('CallDate', StringType(), True),\n",
    "    StructField('WatchDate', StringType(), True),\n",
    "    StructField('CallFinalDisposition', StringType(), True),\n",
    "    StructField('AvailableDtTm', StringType(), True),\n",
    "    StructField('Address', StringType(), True),\n",
    "    StructField('City', StringType(), True),\n",
    "    StructField('Zipcode', IntegerType(), True),\n",
    "    StructField('Battalion', StringType(), True),\n",
    "    StructField('StationArea', StringType(), True),\n",
    "    StructField('Box', StringType(), True),\n",
    "    StructField('OriginalPriority', StringType(), True),\n",
    "    StructField('Priority', StringType(), True),\n",
    "    StructField('FinalPriority', IntegerType(), True),\n",
    "    StructField('ALSUnit', BooleanType(), True),\n",
    "    StructField('CallTypeGroup', StringType(), True),\n",
    "    StructField('NumAlarms', IntegerType(), True),\n",
    "    StructField('UnitType', StringType(), True),\n",
    "    StructField('UnitSequenceInCallDispatch', IntegerType(), True),\n",
    "    StructField('FirePreventionDistrict', StringType(), True),\n",
    "    StructField('SupervisorDistrict', StringType(), True),\n",
    "    StructField('Neighborhood', StringType(), True),\n",
    "    StructField('Location', StringType(), True),\n",
    "    StructField('RowID', StringType(), True),\n",
    "    StructField('Delay', FloatType(), True)\n",
    "])\n",
    "\n",
    "sf_fire_path = './data/sf-fire-calls.csv'\n",
    "\n",
    "# Use the DataFrameReader interface to read a CSV file\n",
    "fire_df = spark.read.csv(sf_fire_path, header=True, schema=fire_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------------+----------+----+-------+\n|        CallType|  CallDate|City|Zipcode|\n+----------------+----------+----+-------+\n|  Structure Fire|01/11/2002|  SF|  94109|\n|Medical Incident|01/11/2002|  SF|  94124|\n|Medical Incident|01/11/2002|  SF|  94102|\n|    Vehicle Fire|01/11/2002|  SF|  94110|\n|          Alarms|01/11/2002|  SF|  94109|\n+----------------+----------+----+-------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "fire_df.select('CallType','CallDate','City','Zipcode').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `spark.read.csv()` function reads in the CSV file and returns a DataFrame of rows and named columns with the types dictated in the schema.\n",
    "\n",
    "To write the DataFrame into an external data source in your format of choice, you can use the **DataFrameWriter** interface. \n",
    "\n",
    "Like **DataFrameReader**, it supports multiple data sources. **Parquet**, a popular columnar format, is the default format; it uses snappy compression to compress the data. \n",
    "\n",
    "**If the DataFrame is written as Parquet, the schema is preserved as part of the Parquet metadata.** In this case, subsequent reads back into a DataFrame do not require you to manually supply a schema.\n",
    "\n",
    "A common data operation is to explore and transform your data, and then persist the DataFrame in Parquet format or save it\n",
    "as a SQL table. Persisting a transformed DataFrame is as easy as reading it. \n",
    "\n",
    "For example, to persist the DataFrame we were just working with as a file after reading it you would do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+----------+---------------------+-------+\n|UnitID|CallDate  |Neighborhood         |Zipcode|\n+------+----------+---------------------+-------+\n|M17   |01/11/2002|Bayview Hunters Point|94124  |\n|M41   |01/11/2002|Tenderloin           |94102  |\n|E05   |01/11/2002|Japantown            |94115  |\n|E06   |01/11/2002|Castro/Upper Market  |94114  |\n|M07   |01/11/2002|Mission              |94110  |\n+------+----------+---------------------+-------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "    fire_df\n",
    "    .where(F.col('CallType')=='Medical Incident')\n",
    "    .select('UnitID', 'CallDate', 'Neighborhood','Zipcode')\n",
    ")\n",
    "\n",
    "df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet('./output/sf-fire-calls.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = './datasets/sf-fire-calls.parquet'\n",
    "# fire_df.write.format('parquet').save(parquet_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can save it as a table, which registers metadata with the Hive metastore (we will cover SQL managed and unmanaged tables, metastores, and DataFrames in the next chapter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquet_table = \n",
    "# fire_df.write.format('parquet').saveAsTable(parquet_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations and actions\n",
    "\n",
    "Now that you have a distributed DataFrame composed of San Francisco Fire Department calls in memory, the first thing you as a developer will want to do is examine your data to see what the columns look like. \n",
    "\n",
    "Are they of the correct types? Do any of them need to be converted to different types? Do they have null values?\n",
    "\n",
    "#### Projections and filters\n",
    "\n",
    "A projection in relational parlance is a way to return only the rows matching a certain relational condition by using filters. In Spark, projections are done with the `select()` method, while filters can be expressed using the `filter()` or `where()` method. \n",
    "\n",
    "We can use this technique to examine specific aspects of our SF Fire Department data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------+----------------------+--------------+\n|IncidentNumber|AvailableDtTm         |CallType      |\n+--------------+----------------------+--------------+\n|2003235       |01/11/2002 01:51:44 AM|Structure Fire|\n|2003250       |01/11/2002 04:16:46 AM|Vehicle Fire  |\n|2003259       |01/11/2002 06:01:58 AM|Alarms        |\n|2003279       |01/11/2002 08:03:26 AM|Structure Fire|\n|2003301       |01/11/2002 09:46:44 AM|Alarms        |\n+--------------+----------------------+--------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "few_fire_df = (fire_df\n",
    "               .select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\")\n",
    "               .where(F.col(\"CallType\") != \"Medical Incident\"))\n",
    "\n",
    "few_fire_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------------+\n|DistinctCallTypes|\n+-----------------+\n|               30|\n+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# return number of distinct types of calls using countDistinct()\n",
    "(fire_df\n",
    " .select(\"CallType\")\n",
    " .where(F.col(\"CallType\").isNotNull())\n",
    " .agg(F.countDistinct(\"CallType\").alias(\"DistinctCallTypes\"))\n",
    " .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------------------------------+\n|CallType                           |\n+-----------------------------------+\n|Elevator / Escalator Rescue        |\n|Marine Fire                        |\n|Aircraft Emergency                 |\n|Confined Space / Structure Collapse|\n|Administrative                     |\n|Alarms                             |\n|Odor (Strange / Unknown)           |\n|Citizen Assist / Service Call      |\n|HazMat                             |\n|Watercraft in Distress             |\n+-----------------------------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# filter for only distinct non-null CallTypes from all the rows\n",
    "(fire_df\n",
    " .select(\"CallType\")\n",
    " .where(F.col(\"CallType\").isNotNull())\n",
    " .distinct()\n",
    " .show(10, False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renaming, adding and dropping columns\n",
    "\n",
    "By specifying the desired column names in the schema with StructField, we effectively changed all names in the resulting DataFrame.\n",
    "\n",
    "Alternatively, you could selectively rename columns with the `withColumnRenamed()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------------------+\n|ResponseDelayedinMins|\n+---------------------+\n|5.35                 |\n|6.25                 |\n|5.2                  |\n|5.6                  |\n|7.25                 |\n+---------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "new_fire_df = fire_df.withColumnRenamed(\"Delay\", \"ResponseDelayedinMins\")\n",
    "\n",
    "(new_fire_df\n",
    ".select(\"ResponseDelayedinMins\")\n",
    ".where(F.col(\"ResponseDelayedinMins\") > 5)\n",
    ".show(5, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because DataFrame transformations are immutable, when we rename a column using withColumnRenamed() we get a new DataFrame while retaining the original with the old column name.\n",
    "\n",
    "Modifying the contents of a column or its type are common operations during data exploration.\n",
    "\n",
    "For example, in our SF Fire Department data set, the columns CallDate, WatchDate, and AlarmDtTm are strings rather than either Unix timestamps or SQL dates, both of which Spark supports and can easily manipulate during transformations or actions (e.g., during a date- or timebased analysis of the data).\n",
    "\n",
    "So how do we convert them into a more usable format? It’s quite simple, thanks to some high-level API methods. `spark.sql.functions` has a set of to/from date/timestamp functions such as `to_timestamp()` and `to_date()` that we can use for just this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+----------+----------------------+\n|CallDate  |WatchDate |AvailableDtTm         |\n+----------+----------+----------------------+\n|01/11/2002|01/10/2002|01/11/2002 01:51:44 AM|\n|01/11/2002|01/10/2002|01/11/2002 03:01:18 AM|\n|01/11/2002|01/10/2002|01/11/2002 02:39:50 AM|\n+----------+----------+----------------------+\nonly showing top 3 rows\n\n[('CallDate', 'string'), ('WatchDate', 'string'), ('AvailableDtTm', 'string')]\n"
     ]
    }
   ],
   "source": [
    "new_fire_df.select(\"CallDate\", \"WatchDate\", \"AvailableDtTm\").show(3, False)\n",
    "\n",
    "print(new_fire_df.select(\"CallDate\", \"WatchDate\", \"AvailableDtTm\").dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------------+-------------------+-------------------+\n|IncidentDate       |OnWatchDate        |AvailableDtTS      |\n+-------------------+-------------------+-------------------+\n|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 01:51:44|\n|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 03:01:18|\n|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 02:39:50|\n|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 04:16:46|\n|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 06:01:58|\n+-------------------+-------------------+-------------------+\nonly showing top 5 rows\n\n[('IncidentDate', 'timestamp'), ('OnWatchDate', 'timestamp'), ('AvailableDtTS', 'timestamp')]\n"
     ]
    }
   ],
   "source": [
    "fire_ts_df = (\n",
    "    new_fire_df\n",
    "    .withColumn(\"IncidentDate\", F.to_timestamp(F.col(\"CallDate\"), \"MM/dd/yyyy\")).drop(\"CallDate\")\n",
    "    .withColumn(\"OnWatchDate\", F.to_timestamp(F.col(\"WatchDate\"), \"MM/dd/yyyy\")).drop(\"WatchDate\")\n",
    "    .withColumn(\"AvailableDtTS\", F.to_timestamp(F.col(\"AvailableDtTm\"),\"MM/dd/yyyy hh:mm:ss a\")).drop(\"AvailableDtTm\")\n",
    ")\n",
    "\n",
    "# Select the converted columns\n",
    "fire_ts_df.select(\"IncidentDate\", \"OnWatchDate\", \"AvailableDtTS\").show(5, False)\n",
    "\n",
    "print(fire_ts_df.select(\"IncidentDate\", \"OnWatchDate\", \"AvailableDtTS\").dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have modified the dates, we can query using functions from spark.sql.functions like `month()`, `year()`, and `day()` to explore our data further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------+\n|year(IncidentDate)|\n+------------------+\n|              2000|\n|              2001|\n|              2002|\n|              2003|\n|              2004|\n|              2005|\n|              2006|\n|              2007|\n|              2008|\n|              2009|\n|              2010|\n|              2011|\n|              2012|\n|              2013|\n|              2014|\n|              2015|\n|              2016|\n|              2017|\n|              2018|\n+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "(fire_ts_df\n",
    " .select(F.year('IncidentDate'))\n",
    " .distinct()\n",
    " .orderBy(F.year('IncidentDate'))\n",
    " .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregations\n",
    "\n",
    "A handful of transformations and actions on DataFrames, such as `groupBy()`, `orderBy()`, and `count()`, offer the ability to aggregate by column names and then aggregate counts across them.\n",
    "\n",
    "For larger DataFrames on which you plan to conduct frequent or repeated queries, you could benefit from caching. We will cover\n",
    "DataFrame caching strategies and their benefits in later chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------------------------+------+\n|CallType                       |count |\n+-------------------------------+------+\n|Medical Incident               |113794|\n|Structure Fire                 |23319 |\n|Alarms                         |19406 |\n|Traffic Collision              |7013  |\n|Citizen Assist / Service Call  |2524  |\n|Other                          |2166  |\n|Outside Fire                   |2094  |\n|Vehicle Fire                   |854   |\n|Gas Leak (Natural and LP Gases)|764   |\n|Water Rescue                   |755   |\n+-------------------------------+------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# what were the most common types of fire calls?\n",
    "(fire_ts_df\n",
    " .select(\"CallType\")\n",
    " .where(F.col(\"CallType\").isNotNull())\n",
    " .groupBy(\"CallType\")\n",
    " .count()\n",
    " .orderBy(\"count\", ascending=False)\n",
    " .show(n=10, truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame API also offers the `collect()` method, but for extremely large DataFrames this is resource-heavy (expensive) and\n",
    "dangerous, as it can cause out-of-memory (OOM) exceptions.\n",
    "\n",
    "Unlike `count()`, which returns a single number to the driver, `collect()` returns a collection of all the Row objects in the entire DataFrame or Dataset. \n",
    "\n",
    "If you want to take a peek at some Row records you’re better off with `take(n)`, which will return only the first n Row objects of the DataFrame.\n",
    "\n",
    "#### Other common DataFrame operations\n",
    "\n",
    "Along with all the others we’ve seen, the DataFrame API provides descriptive statistical methods like `min()`, `max()`, `sum()` and `avg()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------+--------------------------+--------------------------+--------------------------+\n|sum(NumAlarms)|avg(ResponseDelayedinMins)|min(ResponseDelayedinMins)|max(ResponseDelayedinMins)|\n+--------------+--------------------------+--------------------------+--------------------------+\n|        176170|         3.892364154521585|               0.016666668|                   1844.55|\n+--------------+--------------------------+--------------------------+--------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "(fire_ts_df.select(F.sum(\"NumAlarms\"), \n",
    "                   F.avg(\"ResponseDelayedinMins\"),\n",
    "                   F.min(\"ResponseDelayedinMins\"), \n",
    "                   F.max(\"ResponseDelayedinMins\"))\n",
    ".show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-End DataFrame Example\n",
    "\n",
    "* What were all the different types of fire calls in 2018?\n",
    "* What months within the year 2018 saw the highest number of fire calls?\n",
    "* Which neighborhood in San Francisco generated the most fire calls in 2018?\n",
    "* Which neighborhoods had the worst response times to fire calls in 2018?\n",
    "* Which week in the year in 2018 had the most fire calls?\n",
    "* Is there a correlation between neighborhood, zip code, and number of fire calls?\n",
    "* How can we use Parquet files or SQL tables to store this data and read it back?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------------------------+-----+\n|CallType                       |count|\n+-------------------------------+-----+\n|Medical Incident               |7004 |\n|Alarms                         |1144 |\n|Structure Fire                 |906  |\n|Traffic Collision              |433  |\n|Outside Fire                   |153  |\n|Other                          |114  |\n|Citizen Assist / Service Call  |113  |\n|Gas Leak (Natural and LP Gases)|69   |\n|Water Rescue                   |43   |\n|Elevator / Escalator Rescue    |36   |\n|Electrical Hazard              |30   |\n|Smoke Investigation (Outside)  |28   |\n|Vehicle Fire                   |28   |\n|Odor (Strange / Unknown)       |10   |\n|Fuel Spill                     |10   |\n|HazMat                         |5    |\n|Train / Rail Incident          |5    |\n|Suspicious Package             |3    |\n|Explosion                      |1    |\n|Assist Police                  |1    |\n+-------------------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# What were all the different types of fire calls in 2018?\n",
    "\n",
    "(fire_ts_df.select('CallType')\n",
    " .where((F.col('CallType').isNotNull()) & (F.year('IncidentDate')==2018))\n",
    " .groupBy('CallType')\n",
    " .count()\n",
    " .orderBy('count', ascending=False)\n",
    " .show(truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------------+-----+\n|month(IncidentDate)|count|\n+-------------------+-----+\n|10                 |1068 |\n|5                  |1047 |\n|3                  |1029 |\n|8                  |1021 |\n|1                  |1007 |\n|7                  |974  |\n|6                  |974  |\n|9                  |951  |\n|4                  |947  |\n|2                  |919  |\n|11                 |199  |\n+-------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# What months within the year 2018 saw the highest number of fire calls?\n",
    "\n",
    "(fire_ts_df.select('IncidentDate')\n",
    " .where(F.year('IncidentDate')==2018)\n",
    " .groupBy(F.month('IncidentDate'))\n",
    " .count()\n",
    " .orderBy('count', ascending=False)\n",
    " .show(truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------------------+-----+\n|Neighborhood                  |count|\n+------------------------------+-----+\n|Tenderloin                    |1393 |\n|South of Market               |1053 |\n|Mission                       |913  |\n|Financial District/South Beach|772  |\n|Bayview Hunters Point         |522  |\n+------------------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Which neighborhood in San Francisco generated the most fire calls in 2018?\n",
    "\n",
    "(fire_ts_df\n",
    " .where(F.year('IncidentDate')==2018)\n",
    " .groupBy('Neighborhood')\n",
    " .count()\n",
    " .orderBy('count', ascending=False)\n",
    " .show(5, truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset API\n",
    "\n",
    "Spark 2.0 unified the DataFrame and Dataset APIs as Structured APIs with similar interfaces so that developers would only have to learn a single set of APIs.\n",
    "\n",
    "Datasets take on two characteristics: **typed and untyped APIs**.\n",
    "\n",
    "Conceptually, you can think of a **DataFrame** in Scala as an alias for a collection of generic objects, `Dataset[Row]`, where a Row is a generic untyped JVM object that may hold different types of fields. \n",
    "\n",
    "A **Dataset**, by contrast, is a collection of strongly typed JVM objects in Scala or a class in Java.\n",
    "\n",
    "**Datasets make sense only in Java and Scala**, whereas in Python and R only DataFrames make sense. This is because Python and R are not compile-time type-safe; types are dynamically inferred or assigned during execution, not during compile time.\n",
    "\n",
    "The reverse is true in Scala and Java: types are bound to variables and objects at compile time. In Scala, however, a DataFrame is just an alias for untyped Dataset[Row].\n",
    "\n",
    "https://data-flair.training/blogs/apache-spark-rdd-vs-dataframe-vs-dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL and the Underlying Engine\n",
    "\n",
    "Spark SQL allows developers to issue ANSI SQL:2003–compatible queries on structured data with a schema. Apart from that, the Spark SQL engine:\n",
    "\n",
    "* Unifies Spark components and permits abstraction to DataFrames/Datasets in Java, Scala, Python, and R, which simplifies working with structured data sets.\n",
    "* Connects to the Apache Hive metastore and tables.\n",
    "* Reads and writes structured data with a specific schema from structured file formats (JSON, CSV, Text, Avro, Parquet, ORC, etc.) and converts data into temporary tables.\n",
    "* Offers an interactive Spark SQL shell for quick data exploration.\n",
    "* Provides a bridge to (and from) external tools via standard database JDBC/ODBC connectors.\n",
    "* Generates optimized query plans and compact code for the JVM, for final execution.\n",
    "\n",
    "At the core of the Spark SQL engine are the Catalyst optimizer and Project Tungsten. Together, these support the high-level DataFrame and Dataset APIs and SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "interpreter": {
   "hash": "a49a3eaf43f92c8c35fbc05f2ff363591d97c0b1992a66140feb5af3adc6463c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
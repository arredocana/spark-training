{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4. Spark SQL and DataFrames: Introduction to Built-in Data Sources\n",
    "\n",
    "Spark SQL engine provides a unified foundation for the high-level DataFrame and Dataset APIs.\n",
    "\n",
    "In particular, Spark SQL:\n",
    "\n",
    "* Provides the engine upon which the high-level Structured APIs are built.\n",
    "* Can read and write data in a variety of structured formats (e.g., JSON, Hivetables, Parquet, Avro, ORC, CSV).\n",
    "* Lets you query data using JDBC/ODBC connectors from external business intelligence (BI) data sources such as Tableau, Power BI, Talend, or from RDBMSs such as MySQL and PostgreSQL.\n",
    "* Provides a programmatic interface to interact with structured data stored as tables or views in a database from a Spark application\n",
    "* Offers an interactive shell to issue SQL queries on your structured data.\n",
    "* Supports ANSI SQL:2003-compliant commands and HiveQL.\n",
    "\n",
    "## Using Spark SQL in Spark Applications\n",
    "\n",
    "The **SparkSession**, introduced in Spark 2.0, provides a unified entry point for programming Spark with the Structured APIs. You can use a SparkSession to access Spark functionality: just import the class and create an instance in your code.\n",
    "\n",
    "To issue any SQL query, use the `sql()` method on the SparkSession instance, spark, such as spark.sql(\"SELECT * FROM myTableName\").\n",
    "\n",
    "**All spark.sql queries executed in this manner return a DataFrame** on which you may perform further Spark operations if you desire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PySpark and libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000001F1BCC43040>\n",
      "Spark version:  3.0.1\n"
     ]
    }
   ],
   "source": [
    "# Build a SparkSession\n",
    "spark = SparkSession.builder.appName(\"chapter4\").getOrCreate()\n",
    "\n",
    "# Print SparkSession\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "|01031245|   -4|     602|   ABE|        ATL|\n",
      "|01030605|    0|     602|   ABE|        ATL|\n",
      "|01041243|   10|     602|   ABE|        ATL|\n",
      "|01040605|   28|     602|   ABE|        ATL|\n",
      "|01051245|   88|     602|   ABE|        ATL|\n",
      "|01050605|    9|     602|   ABE|        ATL|\n",
      "|01061215|   -6|     602|   ABE|        ATL|\n",
      "|01061725|   69|     602|   ABE|        ATL|\n",
      "|01061230|    0|     369|   ABE|        DTW|\n",
      "|01060625|   -3|     602|   ABE|        ATL|\n",
      "|01070600|    0|     369|   ABE|        DTW|\n",
      "|01071725|    0|     602|   ABE|        ATL|\n",
      "|01071230|    0|     369|   ABE|        DTW|\n",
      "|01070625|    0|     602|   ABE|        ATL|\n",
      "|01071219|    0|     569|   ABE|        ORD|\n",
      "|01080600|    0|     369|   ABE|        DTW|\n",
      "+--------+-----+--------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to data set\n",
    "file_path = \"./datasets/departure_delays.csv\"\n",
    "\n",
    "# Define schema\n",
    "flights_schema = StructType([\n",
    "    StructField(\"date\", StringType()),\n",
    "    StructField(\"delay\", IntegerType()),\n",
    "    StructField(\"distance\", IntegerType()),\n",
    "    StructField(\"origin\", StringType()),\n",
    "    StructField(\"destination\", StringType())\n",
    "])\n",
    "\n",
    "# Create a DataFrame\n",
    "us_delay_flights_df = spark.read.csv(path= file_path,\n",
    "                                     header=True,\n",
    "                                     schema=flights_schema,\n",
    "                                     timestampFormat=\"mm-yy hh:mm a\",)\n",
    "\n",
    "\n",
    "# Show the 20 first rows of the dataset\n",
    "us_delay_flights_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table(name='delay_flights_tbl', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
     ]
    }
   ],
   "source": [
    "# Create a temporal table from DataFrame\n",
    "us_delay_flights_df.createOrReplaceTempView(\"delay_flights_tbl\")\n",
    "\n",
    "# Print the tables in the catalog\n",
    "print(spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------------------+-----------+\n",
      "|namespace|viewName                      |isTemporary|\n",
      "+---------+------------------------------+-----------+\n",
      "|         |delay_flights_tbl             |true       |\n",
      "|         |us_origin_airport_jfk_tmp_view|true       |\n",
      "+---------+------------------------------+-----------+\n",
      "\n",
      "+--------------+\n",
      "|namespace     |\n",
      "+--------------+\n",
      "|default       |\n",
      "|learn_spark_db|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show tables and views\n",
    "spark.sql(\"SHOW VIEWS\").show(truncate=False)\n",
    "spark.sql(\"SHOW DATABASES\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+-----------+\n",
      "|namespace|viewName         |isTemporary|\n",
      "+---------+-----------------+-----------+\n",
      "|         |delay_flights_tbl|true       |\n",
      "+---------+-----------------+-----------+\n",
      "\n",
      "+--------------+-------------------------------+-----------+\n",
      "|database      |tableName                      |isTemporary|\n",
      "+--------------+-------------------------------+-----------+\n",
      "|learn_spark_db|df_managed_us_delay_flights_tbl|false      |\n",
      "|learn_spark_db|unmanaged_us_delay_flights_tbl |false      |\n",
      "|              |delay_flights_tbl              |true       |\n",
      "+--------------+-------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show tables and views\n",
    "spark.sql(\"SHOW VIEWS\").show(truncate=False)\n",
    "spark.sql(\"SHOW TABLES\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    SELECT distance, origin, destination\n",
    "    FROM delay_flights_tbl\n",
    "    WHERE distance > 1000\n",
    "    ORDER BY distance DESC;\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show(10)\n",
    "print(type(spark.sql(query)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "(us_delay_flights_df.select(\"distance\", \"origin\", \"destination\")\n",
    "    .where(\"distance > 1000\") #(F.col(\"distance\") > 1000)\n",
    "    .orderBy(\"distance\", ascending=False) #(F.desc(\"distance\"))\n",
    "    .show(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = \"\"\"\n",
    "    SELECT date, delay, origin, destination\n",
    "    FROM delay_flights_tbl\n",
    "    WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD'\n",
    "    ORDER by delay DESC;\n",
    "\"\"\"\n",
    "spark.sql(query2).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable you to query structured data as shown in the preceding examples, Spark manages all the complexities of creating and managing views and tables, both in memory and on disk. That leads us to our next topic: how tables and views are created\n",
    "and managed.\n",
    "\n",
    "## SQL Tables and Views\n",
    "\n",
    "Tables hold data. Associated with each table in Spark is its relevant **metadata**, which is information about the table and its data: the schema, description, table name, database name, column names, partitions, physical location where the actual data resides,etc. All of this is stored in a central metastore.\n",
    "\n",
    "Instead of having a separate metastore for Spark tables, Spark by default uses the **Apache Hive metastore**, located at /user/hive/warehouse, to persist all the metadata about your tables. \n",
    "\n",
    "However, you may change the default location by setting the Spark config variable spark.sql.warehouse.dir to another location, which can be set to a local or external distributed storage.\n",
    "\n",
    "### Managed vs Unmanaged Tables\n",
    "\n",
    "Spark allows you to create two types of tables: managed and unmanaged. \n",
    "\n",
    "* For a **managed table**, Spark manages both the metadata and the data in the file store. This could be a local filesystem, HDFS, or an object store such as Amazon S3 or Azure Blob.\n",
    "\n",
    "* For an **unmanaged table**, Spark only manages the metadata, while you manage the data yourself in an external data source such as Cassandra.\n",
    "\n",
    "With a **managed table**, because Spark manages everything, a SQL command such as `DROP TABLE table_name` deletes both the metadata and the data. With an **unmanaged table**, the same command will delete only the metadata, not the actual data.\n",
    "\n",
    "### Creating SQL Databases and Tables\n",
    "\n",
    "Tables reside within a database. By default, Spark creates tables under the *default* database. To create your own database name, you can issue a SQL command from your Spark application or notebook. \n",
    "\n",
    "Using the US flight delays data set, let’s create both a managed and an unmanaged table. To begin, we’ll create a database called learn_spark_db and tell Spark we want to use that database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|     namespace|\n",
      "+--------------+\n",
      "|       default|\n",
      "|learn_spark_db|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a database\n",
    "spark.sql(\"CREATE DATABASE learn_spark_db\")\n",
    "\n",
    "# Use the new database created\n",
    "spark.sql(\"USE learn_spark_db\")\n",
    "\n",
    "# Check databases in Spark\n",
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a managed table\n",
    "\n",
    "To create a managed table within the database learn_spark_db, you can issue a SQL query like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create managed table using SQL query\n",
    "query = \"\"\" \n",
    "    CREATE TABLE managed_us_delay_flights_tbl (\n",
    "    date STRING, \n",
    "    delay INT, \n",
    "    distance INT, \n",
    "    origin STRING, \n",
    "    destination STRING;)\n",
    "\"\"\"\n",
    "# AnalysisException: Hive support is required to CREATE Hive TABLE (AS SELECT);;\n",
    "#spark.sql(query)\n",
    "\n",
    "# Create managed table using DataFrame API\n",
    "us_delay_flights_df.write.saveAsTable(\"df_us_delay_flights_managed_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------------------------+-----------+\n",
      "|database      |tableName                          |isTemporary|\n",
      "+--------------+-----------------------------------+-----------+\n",
      "|learn_spark_db|df_us_delay_flights_managed_table  |false      |\n",
      "|learn_spark_db|df_us_delay_flights_unmanaged_table|false      |\n",
      "|              |delay_flights_tbl                  |true       |\n",
      "+--------------+-----------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------------------+-----------+\n",
      "|database      |tableName                      |isTemporary|\n",
      "+--------------+-------------------------------+-----------+\n",
      "|learn_spark_db|df_managed_us_delay_flights_tbl|false      |\n",
      "|              |delay_flights_tbl              |true       |\n",
      "+--------------+-------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show tables\n",
    "spark.sql(\"SHOW TABLES\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|02151800|  108|     290|   ORD|        MSP|\n",
      "|02151800|  142|     772|   ORD|        DEN|\n",
      "|02151303|   16|    1516|   ORD|        LAX|\n",
      "|02151157|    7|    1316|   ORD|        LAS|\n",
      "|02151818|   55|    1511|   ORD|        PDX|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM df_us_delay_flights_managed_table LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating an unmanaged table\n",
    "\n",
    "Also you can create unmanaged tables from your own data sources—say, Parquet, CSV, or JSON files stored in a file store accessible to your Spark application.\n",
    "\n",
    "To create an unmanaged table from a data source such as a CSV file, in SQL use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create unmanaged table using SQL query\n",
    "query = \"\"\" \n",
    "    CREATE TABLE df_us_delay_flights_unmanaged_table (\n",
    "    date STRING, \n",
    "    delay INT, \n",
    "    distance INT, \n",
    "    origin STRING, \n",
    "    destination STRING )\n",
    "    USING CSV\n",
    "    OPTIONS (PATH \"./datasets/departure_delays.csv\", HEADER \"TRUE\");\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------------------+-----------+\n",
      "|database      |tableName                      |isTemporary|\n",
      "+--------------+-------------------------------+-----------+\n",
      "|learn_spark_db|df_managed_us_delay_flights_tbl|false      |\n",
      "|learn_spark_db|unmanaged_us_delay_flights_tbl |false      |\n",
      "|              |delay_flights_tbl              |true       |\n",
      "+--------------+-------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show tables in the learn_spark_db database\n",
    "spark.sql(\"SHOW TABLES\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "|01031245|   -4|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM unmanaged_us_delay_flights_tbl LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unmanaged table using DataFrame API\n",
    "#(us_delay_flights_df\n",
    "# .write\n",
    "# .option(\"path\", file_path)\n",
    "# .saveAsTable(\"df_unmanaged_us_delay_flights_tbl\")\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"DROP TABLE unmanaged_us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Views\n",
    "\n",
    "In addition to creating tables, Spark can create **views** on top of existing tables. \n",
    "\n",
    "Views can be **global** (visible across all SparkSessions on a given cluster) or **session-scoped** (visible only to a single SparkSession), and they are **temporary**: they disappear after your Spark application terminates.\n",
    "\n",
    "Creating views has a similar syntax to creating tables within a database. Once you create a view, you can query it as you would a table. \n",
    "\n",
    "**The difference between a view and a table is that views don’t actually hold the data; tables persist after your Spark application terminates, but views disappear**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In SQL\n",
    "query1 = \"\"\"\n",
    "    CREATE OR REPLACE GLOBAL TEMP VIEW us_origin_airport_SFO_global_tmp_view AS\n",
    "    SELECT date, origin, destination \n",
    "    FROM delay_flights_tbl\n",
    "    WHERE origin=\"SFO\"\n",
    "\"\"\"\n",
    "query2 = \"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW us_origin_airport_JFK_tmp_view AS\n",
    "    SELECT date, origin, destination \n",
    "    FROM delay_flights_tbl\n",
    "    WHERE origin=\"JFK\"\n",
    "\"\"\"\n",
    "\n",
    "# In Python\n",
    "df_sfo = spark.sql(\"SELECT date, delay, origin, destination FROM delay_flights_tbl WHERE origin = 'SFO'\")\n",
    "df_jfk = spark.sql(\"SELECT date, delay, origin, destination FROM delay_flights_tbl WHERE origin = 'JFK'\")\n",
    "\n",
    "# Create a temporary and global temporary view\n",
    "df_sfo.createOrReplaceGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "df_jfk.createOrReplaceTempView(\"us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='us_origin_airport_sfo_global_tmp_view', database='global_temp', description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='delay_flights_tbl', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='us_origin_airport_jfk_tmp_view', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables(\"global_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------------------------+-----------+\n",
      "|database      |tableName                          |isTemporary|\n",
      "+--------------+-----------------------------------+-----------+\n",
      "|learn_spark_db|df_us_delay_flights_managed_table  |false      |\n",
      "|learn_spark_db|df_us_delay_flights_unmanaged_table|false      |\n",
      "|              |delay_flights_tbl                  |true       |\n",
      "|              |us_origin_airport_jfk_tmp_view     |true       |\n",
      "+--------------+-----------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you’ve created these views, you can issue queries against them just as you would against a table. Keep in mind that when accessing a global temporary view you must use the prefix `global_temp.<view_name>`, because **Spark creates global temporary\n",
    "views in a global temporary database called global_temp**. \n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|01011250|   55|   SFO|        JFK|\n",
      "|01012230|    0|   SFO|        JFK|\n",
      "|01010705|   -7|   SFO|        JFK|\n",
      "|01010620|   -3|   SFO|        MIA|\n",
      "|01010915|   -3|   SFO|        LAX|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM global_temp.us_origin_airport_SFO_global_tmp_view\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By contrast, you can access the normal temporary view without the `global_temp` prefix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|01010900|   14|   JFK|        LAX|\n",
      "|01011200|   -3|   JFK|        LAX|\n",
      "|01011900|    2|   JFK|        LAX|\n",
      "|01011700|   11|   JFK|        LAS|\n",
      "|01010800|   -1|   JFK|        SFO|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql(\"SELECT * FROM us_origin_airport_JFK_tmp_view\").show(5)\n",
    "spark.read.table(\"us_origin_airport_JFK_tmp_view\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also drop a view just like you would a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In SQL\n",
    "#DROP VIEW IF EXISTS us_origin_airport_SFO_global_tmp_view;\n",
    "#DROP VIEW IF EXISTS us_origin_airport_JFK_tmp_view;\n",
    "\n",
    "# In Python\n",
    "#spark.catalog.dropGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "#spark.catalog.dropTempView(\"us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporary views versus global temporary views\n",
    "\n",
    "A **temporary view** is tied to a single SparkSession within a Spark application. \n",
    "\n",
    "In contrast, a **global temporary** view is visible across multiple SparkSessions within a Spark application. \n",
    "\n",
    "Yes, you can create multiple SparkSessions within a single Spark application—this can be handy, for example, in cases where you want to access (and combine) data from two different SparkSessions that don’t share the same Hive metastore configurations.\n",
    "\n",
    "### Viewing the Metadata\n",
    "\n",
    "Spark manages the metadata associated with each managed or unmanaged table. This is captured in the **Catalog**, a high-level abstraction in Spark SQL for storing metadata. \n",
    "\n",
    "The Catalog’s functionality was expanded in Spark 2.x with new public methods enabling you to examine the metadata associated with your databases, tables, and views. Spark 3.0 extends it to use external catalog.\n",
    "\n",
    "For example, within a Spark application, after creating the SparkSession variable spark, you can access all the stored metadata through methods like these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='default database', locationUri='file:/C:/Users/antonio.arredondo/OneDrive%20-%20Bosonit/Spark%20Training/spark-warehouse'),\n",
       " Database(name='learn_spark_db', description='', locationUri='file:/C:/Users/antonio.arredondo/OneDrive%20-%20Bosonit/Spark%20Training/spark-warehouse/learn_spark_db.db')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='df_managed_us_delay_flights_tbl', database='learn_spark_db', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='unmanaged_us_delay_flights_tbl', database='learn_spark_db', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='delay_flights_tbl', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='us_origin_airport_jfk_tmp_view', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='date', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='delay', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='distance', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='origin', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='destination', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listColumns(\"df_managed_us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching SQL Tables\n",
    "\n",
    "Although we will discuss table caching strategies in the next chapter, it’s worth mentioning here that, like DataFrames, you can cache and uncache SQL tables and views.\n",
    "\n",
    "In Spark 3.0, in addition to other options, you can specify a table as LAZY, meaning that it should only be cached when it is first used instead of immediately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In SQL\n",
    "# CACHE [LAZY] TABLE <table-name>\n",
    "# UNCACHE TABLE <table-name>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Tables into DataFrames\n",
    "\n",
    "Often, data engineers build data pipelines as part of their regular data ingestion and ETL processes. They populate Spark SQL databases and tables with cleansed data for consumption by applications downstream.\n",
    "\n",
    "Let’s assume you have an existing database, `learn_spark_db`, and table, `us_delay_flights_tbl`, ready for use. Instead of reading from an external JSON file, you can simply use SQL to query the table and assign the returned result to a\n",
    "DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In Python\n",
    "us_flights_df = spark.sql(\"SELECT * FROM delay_flights_tbl\")\n",
    "us_flights_df2 = spark.table(\"delay_flights_tbl\")\n",
    "\n",
    "us_flights_df.show(3)\n",
    "us_flights_df2.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a cleansed DataFrame read from an existing Spark SQL table. You can also read data in other formats using Spark’s built-in data sources, giving you the flexibility to interact with various common file formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources for DataFrames and SQL Tables\n",
    "\n",
    "Spark SQL provides an interface to a variety of data sources. It also provides a set of common methods for reading and writing data to and from these data sources using the Data Sources API.\n",
    "\n",
    "In this section we will cover some of the [built-in data sources](https://spark.apache.org/docs/latest/sql-data-sources.html#data-sources), available file formats, and ways to load and write data, along with specific options pertaining to these data sources. \n",
    "\n",
    "But first, let’s take a closer look at two high-level Data Source API constructs that dictate the manner in which you interact with different data sources: **DataFrameReader** and **DataFrameWriter**.\n",
    "\n",
    "### DataFrameReader\n",
    "\n",
    "DataFrameReader is the core construct for reading data from a data source into a DataFrame. It has a defined format and a recommended pattern for usage:\n",
    "\n",
    "`DataFrameReader.format(args).option(\"key\", \"value\").schema(args).load()`\n",
    "\n",
    "Note that you can only access a DataFrameReader through a SparkSession instance. That is, you cannot create an instance of DataFrameReader. To get an instance handle to it, use:\n",
    "\n",
    "`SparkSession.read`\n",
    "or\n",
    "`SparkSession.readStream`\n",
    "\n",
    "While read returns a handle to DataFrameReader to read into a DataFrame from a static data source, readStream returns an instance to read from a streaming source.\n",
    "\n",
    "We won’t comprehensively enumerate all the different combinations of arguments and options, the [documentation for Python, Scala, R, and Java](https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#manually-specifying-options) offers suggestions and guidance.\n",
    "\n",
    "In general, no schema is needed when reading from a static Parquet data source—the Parquet metadata usually contains the schema, so it’s inferred. However, for streaming data sources you will have to provide a schema.\n",
    "\n",
    "Parquet is the default and preferred data source for Spark because it’s efficient, uses columnar storage, and employs a fast compression algorithm.\n",
    "\n",
    "### DataFrameWriter\n",
    "\n",
    "DataFrameWriter saves or writes data to a specified built-in data source. Unlike with DataFrameReader, you access its instance not from a SparkSession but from the DataFrame you wish to save. It has a few recommended usage patterns:\n",
    "\n",
    "`DataFrameWriter.format(args).option(args).sortBy(args).saveAsTable(table)`\n",
    "\n",
    "More info: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter\n",
    "\n",
    "### Parquet\n",
    "\n",
    "Parquet is the default datasource in Spark. Supported and widely used by many big data processing frameworks and platforms, Parquet is an **open source columnar file format** that offers many I/O optimizations (such as compression, which saves storage space and allows for quick access to data columns).\n",
    "\n",
    "Because of its efficiency and these optimizations, we recommend that after you have transformed and cleansed your data, you save your DataFrames in the Parquet format for downstream consumption.\n",
    "\n",
    "#### Reading Parquet files into a DataFrame\n",
    "\n",
    "Parquet files are stored in a directory structure that contains the data files, metadata, a number of compressed files, and some status files. Metadata in the footer contains the version of the file format, the schema, and column data such as the path, etc.\n",
    "\n",
    "For example, a directory in a Parquet file might contain a set of files like this:\n",
    "\n",
    "_SUCCESS\n",
    "\n",
    "_committed_1799640464332036264\n",
    "\n",
    "_started_1799640464332036264\n",
    "\n",
    "part-00000-tid-1799640464332036264-91273258-d7ef-4dc7-<...>-c000.snappy.parquet\n",
    "\n",
    "There may be a number of part-XXXX compressed files in a directory.\n",
    "\n",
    "To read Parquet files into a DataFrame, you simply specify the format and path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = \"/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet/\"\n",
    "# df = spark.read.format(\"parquet\").load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unless you are reading from a streaming data source there’s no need to supply the schema, because Parquet saves it as part of its metadata.\n",
    "\n",
    "#### Reading Parquet files into a Spark SQL table\n",
    "\n",
    "As well as reading Parquet files into a Spark DataFrame, you can also create a Spark SQL unmanaged table or view directly using SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In SQL\n",
    "\n",
    "q = \"\"\"\n",
    "    CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n",
    "    USING parquet\n",
    "    OPTIONS (path \"/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet/\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing DataFrames to Parquet files\n",
    "\n",
    "Writing or saving a DataFrame as a table or file is a common operation in Spark. To write a DataFrame you simply use the methods and arguments to the DataFrame Writer outlined earlier in this chapter, supplying the location to save the Parquet files to. \n",
    "\n",
    "Recall that Parquet is the default file format. If you don’t include the format() method, the DataFrame will still be saved as a Parquet file.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "#(df.write.format(\"parquet\")\n",
    "#.mode(\"overwrite\")\n",
    "#.option(\"compression\", \"snappy\")\n",
    "#.save(\"/tmp/data/parquet/df_parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a set of compact and compressed Parquet files at the specified path. Since we used snappy as our compression choice here, we’ll have snappy compressed files.\n",
    "\n",
    "#### Writing DataFrames to Spark SQL tables\n",
    "\n",
    "Writing a DataFrame to a SQL table is as easy as writing to a file—just use saveAsTable() instead of save(). This will create a managed table called us_delay_flights_tbl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "#(df.write\n",
    "#.mode(\"overwrite\")\n",
    "#.saveAsTable(\"us_delay_flights_tbl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up, Parquet is the preferred and default built-in data source file format in Spark, and it has been adopted by many other frameworks. We recommend that you use this format in your ETL and data ingestion processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON\n",
    "\n",
    "JavaScript Object Notation (JSON) is also a popular data format. It came to prominence as an easy-to-read and easy-to-parse format compared to XML. It has two representational formats: **single-line mode** and **multiline mode**. Both modes are\n",
    "supported in Spark.\n",
    "\n",
    "In single-line mode each line denotes a single JSON object, whereas in multiline mode the entire multiline object constitutes a single JSON object. To read in this mode, set `multiLine` to true in the `option()` method.\n",
    "\n",
    "More info: https://docs.databricks.com/data/data-sources/read-json.html\n",
    "\n",
    "#### Reading a JSON file into a DataFrame\n",
    "\n",
    "You can read a JSON file into a DataFrame the same way you did with Parquet—just specify \"json\" in the format() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = \"/databricks-datasets/learning-spark-v2/flights/summary-data/json/*\"\n",
    "# df = spark.read.format(\"json\").load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading a JSON file into a Spark SQL table\n",
    "\n",
    "You can also create a SQL table from a JSON file just like you did with Parquet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In SQL\n",
    "q = \"\"\"\n",
    "    CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n",
    "    USING json\n",
    "    OPTIONS (path \"/databricks-datasets/learning-spark-v2/flights/summary-data/json/*\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the table is created, you can read data into a DataFrame using SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing DataFrames to JSON files\n",
    "\n",
    "Saving a DataFrame as a JSON file is simple. Specify the appropriate DataFrameWriter methods and arguments, and supply the location to save the JSON files to. \n",
    "This creates a directory at the specified path populated with a set of compact JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (df.write.format(\"json\")\n",
    "# .mode(\"overwrite\")\n",
    "# .option(\"compression\", \"snappy\")\n",
    "# .save(\"/tmp/data/json/df_json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV\n",
    "\n",
    "As widely used as plain text files, this common text file format captures each datum or field delimited by a comma; each line with comma-separated fields represents a record. \n",
    "\n",
    "Even though a comma is the default separator, you may use other delimiters to separate fields in cases where commas are part of your data. Popular spreadsheets can generate CSV files, so it’s a popular format among data and business analysts.\n",
    "\n",
    "#### Reading a CSV file into a DataFrame\n",
    "\n",
    "As with the other built-in data sources, you can use the DataFrameReader methods and arguments to read a CSV file into a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = \"/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\"\n",
    "# schema = \"DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count INT\"\n",
    "# df = (spark.read.format(\"csv\")\n",
    "# .option(\"header\", \"true\")\n",
    "# .schema(schema)\n",
    "# .option(\"mode\", \"FAILFAST\") # Exit if any errors\n",
    "# .option(\"nullValue\", \"\") # Replace any null data field with quotes\n",
    "# .load(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading a CSV file into a Spark SQL table\n",
    "\n",
    "Creating a SQL table from a CSV data source is no different from using Parquet or JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "    CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n",
    "    USING csv\n",
    "    OPTIONS (\n",
    "        path \"/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\",\n",
    "        header \"true\",\n",
    "        inferSchema \"true\",\n",
    "        mode \"FAILFAST\"\n",
    "        )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you’ve created the table, you can read data into a DataFrame using SQL as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing DataFrames to CSV files\n",
    "\n",
    "Saving a DataFrame as a CSV file is simple. Specify the appropriate DataFrameWriter methods and arguments, and supply the location to save the CSV files to.\n",
    "\n",
    "This generates a folder at the specified location, populated with a bunch of compressed and compact files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.write.format(\"csv\").mode(\"overwrite\").save(\"/tmp/data/csv/df_csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avro\n",
    "\n",
    "Avro format is used, for example, by Apache Kafka for message serializing and deserializing. It offers many benefits,\n",
    "including direct mapping to JSON, speed and efficiency, and bindings available for many programming languages.\n",
    "\n",
    "#### Reading an Avro file into a DataFrame\n",
    "\n",
    "Reading an Avro file into a DataFrame using DataFrameReader is consistent in usage with the other data sources we have discussed in this section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read.format(\"avro\").load(\"/databricks-datasets/learning-spark-v2/flights/summary-data/avro/*\")\n",
    "# df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading an Avro file into a Spark SQL table\n",
    "\n",
    "Again, creating SQL tables using an Avro data source is no different from using Parquet, JSON, or CSV. Once you’ve created a table, you can read data into a DataFrame using SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\" \n",
    "    CREATE OR REPLACE TEMPORARY VIEW episode_tbl\n",
    "    USING avro\n",
    "    OPTIONS (path \"/databricks-datasets/learning-spark-v2/flights/summary-data/avro/*\")\n",
    "\n",
    "\"\"\"\n",
    "# spark.sql(\"SELECT * FROM episode_tbl\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing DataFrames to Avro files\n",
    "\n",
    "Writing a DataFrame as an Avro file is simple. As usual, specify the appropriate DataFrameWriter methods and arguments, and supply the location to save the Avro files to. \n",
    "\n",
    "This generates a folder at the specified location, populated with a bunch of compressed and compact files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (df.write\n",
    "# .format(\"avro\")\n",
    "# .mode(\"overwrite\")\n",
    "# .save(\"/tmp/data/avro/df_avro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avro data source options\n",
    "\n",
    "info: https://spark.apache.org/docs/latest/sql-data-sources-avro.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}